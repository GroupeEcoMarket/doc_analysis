# =============================
# API CONFIGURATION
# =============================
API_HOST=0.0.0.0
API_PORT=8000
API_DEBUG=False

# =============================
# REDIS CONFIGURATION
# =============================
# Host Redis pour Dramatiq (broker de messages)
# En Docker: redis (nom du service)
# En local: localhost ou 127.0.0.1
REDIS_HOST=localhost
REDIS_PORT=6379

# =============================
# PATHS & STORAGE (ENVIRONMENT-DEPENDANT)
# =============================
# Chemins de base pour les données d'entrée/sortie
# Ces chemins sont utilisés par l'API et les workers
INPUT_DIR=data/input
OUTPUT_DIR=data/output
PROCESSED_DIR=data/processed

# Répertoire de stockage temporaire pour les workers Dramatiq
# Les fichiers sont stockés ici avec des identifiants uniques et nettoyés automatiquement
# Utilisé pour le partage de fichiers entre l'API et les workers OCR
TEMP_STORAGE_DIR=data/temp_storage

# ==========================================
# MODEL PATHS
# ==========================================
# Répertoire de base pour les modèles ML
# Les modèles de classification seront cherchés dans ce répertoire
MODEL_PATH=models/

# Chemin complet vers le modèle de classification de documents (joblib)
# Le modèle doit être un fichier joblib contenant soit :
# - Un modèle sklearn/lightgbm seul
# - Un dict avec les clés 'model' et 'class_names'
# Si le chemin est relatif, il sera résolu depuis la racine du projet
# En production Docker, ce chemin doit pointer vers le volume monté
CLASSIFICATION_MODEL_PATH=training_data/artifacts/document_classifier.joblib

# =============================
# CLASSIFICATION MODEL CONFIGURATION
# =============================
# Modèle sentence-transformers pour l'extraction d'embeddings sémantiques
# Ce modèle est utilisé pour créer des représentations vectorielles du texte OCR
# Modèles recommandés pour le français :
# - "antoinelouis/french-me5-base" : Rapide, bonne qualité (recommandé)
# - "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2" : Multilingue, plus petit
# En production, ce modèle sera téléchargé automatiquement depuis HuggingFace au premier usage
CLASSIFICATION_EMBEDDING_MODEL=antoinelouis/french-me5-base

# =============================
# TRAINING PATHS (LOCAL ONLY)
# =============================
# Répertoires pour les données d'entraînement
# Utilisés uniquement lors de l'entraînement des modèles (scripts dans training/)
TRAINING_RAW_DIR=training_data/raw
TRAINING_PROCESSED_DIR=training_data/processed
TRAINING_ARTIFACTS_DIR=training_data/artifacts

# =============================
# EXECUTION PARAMETERS
# =============================
# Nombre de processus workers pour l'application principale
#   - Recommandé: 2 pour dev, 4 pour staging, 8 pour prod
#   - Ces workers traitent les tâches de classification et autres opérations
DRAMATIQ_PROCESSES=4
# Nombre de threads par processus pour l'application principale
#   - Recommandé: 1-2 threads par processus
#   - Augmente le parallélisme au sein de chaque processus worker
#   - Note: En production Docker, cette valeur peut être différente (voir docker-compose.prod.yml)
DRAMATIQ_THREADS=1
# Nombre de processus workers pour le service OCR Docker
#   - Recommandé: 2 pour dev, 4 pour prod
OCR_WORKER_PROCESSES=2
# Nombre de threads par processus pour le service OCR Docker
#   - Recommandé: 1 pour dev, 2 pour prod
OCR_WORKER_THREADS=1
# Répertoire de stockage pour le worker OCR (dans le conteneur Docker)
#   - Doit correspondre au volume monté dans docker-compose.yml
OCR_STORAGE_DIR=/app/data/temp_storage

# =============================
# LOGGING
# =============================
LOG_LEVEL=INFO
LOG_FILE=logs/app.log
# Buffer Python désactivé pour un logging en temps réel (recommandé en Docker)
PYTHONUNBUFFERED=1

# =============================
# STORAGE BACKEND
# =============================
# Storage Backend Configuration
# Backend de stockage pour les fichiers temporaires (local, s3, minio)
# Défaut: local (stockage sur le système de fichiers local)
STORAGE_BACKEND=local



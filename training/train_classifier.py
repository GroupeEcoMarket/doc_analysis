"""
Script d'entra√Ænement pour le classifieur de documents.

Ce script :
1. Scanne un dossier de donn√©es d'entra√Ænement (JSON OCR tri√©s par type)
2. Utilise feature_engineering.py pour vectoriser chaque document
3. Entra√Æne un mod√®le scikit-learn ou lightgbm
4. Sauvegarde le mod√®le et un rapport de performance
"""

import json
import argparse
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
import numpy as np
import joblib
from datetime import datetime
from multiprocessing import Pool, cpu_count
from tqdm import tqdm

# Machine Learning
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    accuracy_score,
    f1_score
)
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

# LightGBM (optionnel)
try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False

# Import du module de classification
import sys

# Ajouter le r√©pertoire racine au path pour les imports
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.classification.feature_engineering import FeatureEngineer
from src.pipeline.models import FeaturesOutput, OCRLine
from src.utils.logger import get_logger

logger = get_logger(__name__)


def process_json_file(file_path_tuple):
    """
    Worker function to load and vectorize a single JSON file.
    Initializes its own FeatureEngineer.
    
    Args:
        file_path_tuple: Tuple of (json_file, doc_type, class_idx, semantic_model_name, min_confidence)
    
    Returns:
        Tuple of (embedding, class_idx) or None if error
    """
    json_file, doc_type, class_idx, semantic_model_name, min_confidence = file_path_tuple
    
    try:
        # Chaque worker initialise son propre FeatureEngineer. C'est plus robuste.
        feature_engineer = FeatureEngineer(
            semantic_model_name=semantic_model_name,
            min_confidence=min_confidence
        )
        
        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        embedding = feature_engineer.extract_document_embedding(data)
        
        return (embedding, class_idx)
    except Exception as e:
        # On log l'erreur mais on ne fait pas planter le pool
        logger.warning(f"Erreur lors du traitement de {json_file}: {e}")
        return None


def load_training_data(
    data_dir: str,
    feature_engineer_params: dict,
    num_workers: int = None
) -> Tuple[np.ndarray, np.ndarray, List[str]]:
    """
    Charge et vectorise les donn√©es d'entra√Ænement en parall√®le.
    
    Structure attendue :
    data_dir/
    ‚îú‚îÄ‚îÄ Attestation_CEE/
    ‚îÇ   ‚îú‚îÄ‚îÄ doc1.json
    ‚îÇ   ‚îî‚îÄ‚îÄ ...
    ‚îú‚îÄ‚îÄ Facture/
    ‚îÇ   ‚îî‚îÄ‚îÄ ...
    ‚îî‚îÄ‚îÄ ...
    
    Args:
        data_dir: R√©pertoire contenant les sous-dossiers par type de document.
        feature_engineer_params: Dictionnaire avec les param√®tres pour FeatureEngineer
            (semantic_model_name, min_confidence).
        num_workers: Nombre de processus workers (d√©faut: cpu_count() - 1).
    
    Returns:
        Tuple de (X, y, class_names) :
        - X: Array numpy des embeddings (n_samples, n_features)
        - y: Array numpy des labels (n_samples,)
        - class_names: Liste des noms de classes dans l'ordre
    """
    if num_workers is None:
        num_workers = max(1, cpu_count() - 1)
    
    data_path = Path(data_dir)
    if not data_path.exists():
        raise FileNotFoundError(f"R√©pertoire de donn√©es introuvable: {data_dir}")
    
    # Parcourir les sous-dossiers (chaque sous-dossier = un type de document)
    type_dirs = sorted([d for d in data_path.iterdir() if d.is_dir()])
    
    if not type_dirs:
        raise ValueError(
            f"Aucun sous-dossier trouv√© dans {data_dir}. "
            "Structure attendue: data_dir/TypeDocument/*.json"
        )
    
    print(f"üìÅ D√©couverte de {len(type_dirs)} types de documents")
    
    # Pr√©parer la liste de toutes les t√¢ches √† effectuer
    tasks = []
    class_names = []
    class_to_idx: Dict[str, int] = {}
    
    for type_dir in type_dirs:
        doc_type = type_dir.name
        
        # Ajouter le type √† la liste des classes
        if doc_type not in class_to_idx:
            class_to_idx[doc_type] = len(class_names)
            class_names.append(doc_type)
        
        class_idx = class_to_idx[doc_type]
        
        # Parcourir les fichiers JSON dans ce dossier
        json_files = list(type_dir.glob('*.json'))
        print(f"  {doc_type}: {len(json_files)} fichiers")
        
        for json_file in json_files:
            tasks.append((
                json_file,
                doc_type,
                class_idx,
                feature_engineer_params['semantic_model_name'],
                feature_engineer_params['min_confidence']
            ))
    
    if not tasks:
        raise ValueError("Aucun document .json trouv√© dans les donn√©es d'entra√Ænement")
    
    print(f"\n‚ö° Vectorisation de {len(tasks)} documents en parall√®le avec {num_workers} workers...")
    
    X_list = []
    y_list = []
    
    # Utiliser le Pool pour ex√©cuter les t√¢ches en parall√®le
    with Pool(processes=num_workers) as pool:
        # tqdm ajoute une barre de progression
        # imap_unordered retourne les r√©sultats d√®s qu'ils sont pr√™ts pour un feedback en temps r√©el
        results = list(tqdm(pool.imap_unordered(process_json_file, tasks), total=len(tasks), desc="Vectorisation"))
    
    # Collecter les r√©sultats
    for result in results:
        if result is not None:
            embedding, class_idx = result
            X_list.append(embedding)
            y_list.append(class_idx)
    
    if not X_list:
        raise ValueError("La vectorisation n'a produit aucun r√©sultat valide.")
    
    # Convertir en arrays numpy
    X = np.array(X_list)
    y = np.array(y_list)
    
    print(f"\n‚úÖ Donn√©es charg√©es: {len(X)} documents, {len(class_names)} classes")
    print(f"   Dimensions des embeddings: {X.shape[1]}")
    print(f"   Distribution des classes:")
    for class_name in class_names:
        count = np.sum(y == class_to_idx[class_name])
        print(f"     {class_name}: {count} documents")
    
    return X, y, class_names


def train_model(
    X_train: np.ndarray,
    y_train: np.ndarray,
    model_type: str = 'lightgbm',
    **model_params
) -> Any:
    """
    Entra√Æne un mod√®le de classification.
    
    Args:
        X_train: Features d'entra√Ænement (n_samples, n_features)
        y_train: Labels d'entra√Ænement (n_samples,)
        model_type: Type de mod√®le ('lightgbm', 'logistic_regression', 'random_forest')
        **model_params: Param√®tres additionnels pour le mod√®le
    
    Returns:
        Mod√®le entra√Æn√©
    """
    print(f"\nüîß Entra√Ænement d'un mod√®le {model_type}...")
    
    if model_type == 'lightgbm':
        if not LIGHTGBM_AVAILABLE:
            raise ImportError(
                "LightGBM n'est pas install√©. "
                "Installez-le avec: pip install lightgbm"
            )
        
        # Param√®tres par d√©faut pour LightGBM
        default_params = {
            'objective': 'multiclass',
            'num_class': len(np.unique(y_train)),
            'metric': 'multi_logloss',
            'boosting_type': 'gbdt',
            'num_leaves': 31,
            'learning_rate': 0.05,
            'feature_fraction': 0.9,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'verbose': -1
        }
        default_params.update(model_params)
        
        # Cr√©er le dataset LightGBM
        train_data = lgb.Dataset(X_train, label=y_train)
        
        # Entra√Æner
        model = lgb.train(
            default_params,
            train_data,
            num_boost_round=100,
            valid_sets=[train_data],
            callbacks=[lgb.early_stopping(stopping_rounds=10), lgb.log_evaluation(period=10)]
        )
    
    elif model_type == 'logistic_regression':
        default_params = {
            'max_iter': 1000,
            'multi_class': 'multinomial',
            'solver': 'lbfgs',
            'random_state': 42
        }
        default_params.update(model_params)
        
        model = LogisticRegression(**default_params)
        model.fit(X_train, y_train)
    
    elif model_type == 'random_forest':
        default_params = {
            'n_estimators': 100,
            'max_depth': 10,
            'random_state': 42,
            'n_jobs': -1
        }
        default_params.update(model_params)
        
        model = RandomForestClassifier(**default_params)
        model.fit(X_train, y_train)
    
    else:
        raise ValueError(
            f"Type de mod√®le inconnu: {model_type}. "
            "Choix: 'lightgbm', 'logistic_regression', 'random_forest'"
        )
    
    print("‚úÖ Mod√®le entra√Æn√© avec succ√®s")
    return model


def evaluate_model(
    model: Any,
    X_test: np.ndarray,
    y_test: np.ndarray,
    class_names: List[str],
    model_type: str
) -> Dict[str, Any]:
    """
    √âvalue le mod√®le sur les donn√©es de test.
    
    Args:
        model: Mod√®le entra√Æn√©
        X_test: Features de test
        y_test: Labels de test
        class_names: Noms des classes
        model_type: Type de mod√®le
    
    Returns:
        Dict avec les m√©triques d'√©valuation
    """
    # Pr√©dictions
    if model_type == 'lightgbm':
        y_pred_proba = model.predict(X_test)
        y_pred = np.argmax(y_pred_proba, axis=1)
    else: # Pour les mod√®les sklearn
        y_pred = model.predict(X_test)
    
    # M√©triques
    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average='weighted')

    feature_importance_results = {}
    
    if model_type == 'lightgbm' and hasattr(model, 'feature_importance'):
        feature_importances = model.feature_importance(importance_type='gain')
        
        # La dimension totale est la largeur de X_test. On soustrait les 4 features de position.
        total_features = X_test.shape[1]
        semantic_dim = total_features - 4

        # G√©rer la division par z√©ro
        total_importance = np.sum(feature_importances)
        if total_importance > 0:
            text_importance = np.sum(feature_importances[:semantic_dim])
            pos_importance = np.sum(feature_importances[semantic_dim:])
            
            # Remplir le dictionnaire de r√©sultats
            feature_importance_results = {
                'text_percent': (text_importance / total_importance) * 100,
                'position_percent': (pos_importance / total_importance) * 100,
            }

    # Si c'est un mod√®le sklearn avec feature importances (ex: RandomForest)
    elif hasattr(model, 'feature_importances_'):
        feature_importances = model.feature_importances_
        
        # Calcul dynamique de la dimension s√©mantique
        total_features = X_test.shape[1]
        semantic_dim = total_features - 4
        
        total_importance = np.sum(feature_importances)
        if total_importance > 0:
            text_importance = np.sum(feature_importances[:semantic_dim])
            pos_importance = np.sum(feature_importances[semantic_dim:])
            
            # Remplir le dictionnaire de r√©sultats
            feature_importance_results = {
                'text_percent': (text_importance / total_importance) * 100,
                'position_percent': (pos_importance / total_importance) * 100,
            }

    # Rapport de classification
    report = classification_report(
        y_test,
        y_pred,
        target_names=class_names,
        output_dict=True
    )
    
    # Matrice de confusion
    cm = confusion_matrix(y_test, y_pred)
    
    # CORRECTION : Retourner le dictionnaire, qui sera vide si pas d'importance
    return {
        'accuracy': accuracy,
        'f1_score': f1,
        'classification_report': report,
        'confusion_matrix': cm.tolist(),
        'class_names': class_names,
        'feature_importance': feature_importance_results  
    }


def save_model_and_report(
    model: Any,
    class_names: List[str],
    metrics: Dict[str, Any],
    model_path: str,
    report_path: str
) -> None:
    """
    Sauvegarde le mod√®le et le rapport de performance.
    
    Args:
        model: Mod√®le entra√Æn√©
        class_names: Noms des classes
        metrics: M√©triques d'√©valuation
        model_path: Chemin pour sauvegarder le mod√®le
        report_path: Chemin pour sauvegarder le rapport
    """
    # Sauvegarder le mod√®le avec les noms de classes
    model_data = {
        'model': model,
        'class_names': class_names
    }
    
    model_file = Path(model_path)
    model_file.parent.mkdir(parents=True, exist_ok=True)
    
    joblib.dump(model_data, model_file)
    print(f"üíæ Mod√®le sauvegard√©: {model_path}")
    
    # Sauvegarder le rapport
    report_data = {
        'timestamp': datetime.now().isoformat(),
        'model_path': str(model_path),
        'metrics': {
            'accuracy': metrics['accuracy'],
            'f1_score': metrics['f1_score']
        },
        'classification_report': metrics['classification_report'],
        'confusion_matrix': metrics['confusion_matrix'],
        'class_names': class_names,
        'feature_importances': metrics.get('feature_importance', {})
    }
    
    report_file = Path(report_path)
    report_file.parent.mkdir(parents=True, exist_ok=True)
    
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report_data, f, indent=2, ensure_ascii=False)
    
    print(f"üìä Rapport sauvegard√©: {report_path}")
    
    # Afficher un r√©sum√©
    print("\n" + "="*60)
    print("üìà R√âSUM√â DES PERFORMANCES")
    print("="*60)
    print(f"Accuracy: {metrics['accuracy']:.4f}")
    print(f"F1-Score (weighted): {metrics['f1_score']:.4f}")
    print("\nRapport par classe:")
    for class_name in class_names:
        if class_name in metrics['classification_report']:
            cls_metrics = metrics['classification_report'][class_name]
            print(f"  {class_name}:")
            print(f"    Precision: {cls_metrics.get('precision', 0):.4f}")
            print(f"    Recall: {cls_metrics.get('recall', 0):.4f}")
            print(f"    F1-Score: {cls_metrics.get('f1-score', 0):.4f}")

    feature_importances = metrics.get('feature_importance', {})
    if feature_importances:
        print("\n--- Importance des Features ---")
        print(f"  Importance relative du Texte    : {feature_importances.get('text_percent', 0):.2f}%")
        print(f"  Importance relative de la Position : {feature_importances.get('position_percent', 0):.2f}%")

def main():
    """Point d'entr√©e principal du script d'entra√Ænement."""
    parser = argparse.ArgumentParser(
        description="Entra√Æne un mod√®le de classification de documents"
    )
    parser.add_argument(
        'data_dir',
        type=str,
        help="R√©pertoire contenant les donn√©es d'entra√Ænement (sous-dossiers par type)"
    )
    parser.add_argument(
        '--model-path',
        type=str,
        default='models/document_classifier.joblib',
        help="Chemin pour sauvegarder le mod√®le (d√©faut: models/document_classifier.joblib)"
    )
    parser.add_argument(
        '--report-path',
        type=str,
        default='models/training_report.json',
        help="Chemin pour sauvegarder le rapport (d√©faut: models/training_report.json)"
    )
    parser.add_argument(
        '--model-type',
        type=str,
        default='lightgbm',
        choices=['lightgbm', 'logistic_regression', 'random_forest'],
        help="Type de mod√®le √† entra√Æner (d√©faut: lightgbm)"
    )
    parser.add_argument(
        '--test-size',
        type=float,
        default=0.2,
        help="Proportion des donn√©es pour le test (d√©faut: 0.2)"
    )
    parser.add_argument(
        '--random-state',
        type=int,
        default=42,
        help="Seed al√©atoire pour la reproductibilit√© (d√©faut: 42)"
    )
    parser.add_argument(
        '--semantic-model',
        type=str,
        default='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
        help="Mod√®le sentence-transformers √† utiliser (d√©faut: paraphrase-multilingual-MiniLM-L12-v2)"
    )
    parser.add_argument(
        '--min-confidence',
        type=float,
        default=0.70,
        help="Seuil de confiance minimum pour filtrer les lignes OCR (d√©faut: 0.70)"
    )
    parser.add_argument(
        '--workers',
        '-w',
        type=int,
        default=None,
        help="Nombre de workers parall√®les pour la vectorisation (d√©faut: cpu_count() - 1)"
    )
    
    args = parser.parse_args()
    
    try:
        # On passe les param√®tres √† la place de l'objet FeatureEngineer
        feature_engineer_params = {
            'semantic_model_name': args.semantic_model,
            'min_confidence': args.min_confidence
        }
        
        # Charger et vectoriser les donn√©es en parall√®le
        print("\nüìö Chargement et vectorisation des donn√©es d'entra√Ænement...")
        X, y, class_names = load_training_data(args.data_dir, feature_engineer_params, num_workers=args.workers)
        
        # Split train/test
        print(f"\n‚úÇÔ∏è  Division train/test (test_size={args.test_size})...")
        X_train, X_test, y_train, y_test = train_test_split(
            X, y,
            test_size=args.test_size,
            random_state=args.random_state,
            stratify=y  # Stratifier pour maintenir la distribution des classes
        )
        print(f"   Train: {len(X_train)} √©chantillons")
        print(f"   Test: {len(X_test)} √©chantillons")
        
        # Entra√Æner le mod√®le
        model = train_model(
            X_train,
            y_train,
            model_type=args.model_type
        )
        
        # √âvaluer le mod√®le
        print("\nüìä √âvaluation du mod√®le...")
        metrics = evaluate_model(model, X_test, y_test, class_names, args.model_type)
        
        # Sauvegarder le mod√®le et le rapport
        print("\nüíæ Sauvegarde...")
        save_model_and_report(
            model,
            class_names,
            metrics,
            args.model_path,
            args.report_path
        )
        
        print("\n‚úÖ Entra√Ænement termin√© avec succ√®s !")
        return 0
    
    except Exception as e:
        logger.error(f"Erreur lors de l'entra√Ænement: {e}", exc_info=True)
        print(f"\n‚ùå Erreur: {e}")
        return 1


if __name__ == '__main__':
    exit(main())


"""
API routes for document analysis
"""
import dramatiq
from dramatiq.results.errors import ResultMissing
from dramatiq.results import Results
from src.workers import classify_document_task, warmup_worker_task
from src.utils.ocr_client import warmup_ocr_worker
from dramatiq.results import ResultTimeout

from fastapi import APIRouter, UploadFile, File, HTTPException, BackgroundTasks, Depends
from fastapi.responses import JSONResponse
from typing import Optional, Dict, Any, List, Tuple, Annotated, Union
from pydantic import BaseModel, Field
import os
from pathlib import Path
import numpy as np
import cv2
import time

from src.services.processing_service import ProcessingService
from src.services.task_manager import get_task_manager, TaskStatus
from src.api.dependencies import (
    get_preprocessing_normalizer,
    get_geometry_normalizer,
    get_app_config,
    get_feature_extractor,
    get_document_classifier
)
from src.pipeline.preprocessing import PreprocessingNormalizer
from src.pipeline.geometry import GeometryNormalizer
from src.pipeline.features import FeatureExtractor
from src.pipeline.models import PageClassificationResult
from src.classification.classifier_service import DocumentClassifier
from src.utils.logger import get_logger, get_request_id
from src.utils.exceptions import (
    GeometryError, 
    PreprocessingError, 
    ModelLoadingError, 
    ImageProcessingError,
    PipelineError,
    FeatureExtractionError
)
from src.utils.pdf_handler import is_pdf, pdf_buffer_to_images
from src.utils.config_loader import Config

logger = get_logger(__name__)

router = APIRouter()

# Initialize task manager (singleton)
task_manager = get_task_manager()


# ==========================================
# Pydantic Response Models
# ==========================================

class OutputFiles(BaseModel):
    """Output file paths generated by the pipeline"""
    transformed: str = Field(..., description="Path to the transformed/processed image")
    original: Optional[str] = Field(None, description="Path to the original image (if saved)")
    transform_file: str = Field(..., description="Path to the JSON file containing applied transformations")
    qa_file: str = Field(..., description="Path to the JSON file containing QA flags")


class Metadata(BaseModel):
    """Processing metadata and applied transformations"""
    crop_applied: bool = Field(..., description="Whether intelligent cropping was applied")
    deskew_applied: bool = Field(..., description="Whether angle correction (deskew) was applied")
    rotation_applied: bool = Field(..., description="Whether orientation correction was applied")
    orientation_angle: float = Field(..., description="Detected orientation angle in degrees")
    capture_type: str = Field(..., description="Detected capture type: 'SCAN' or 'PHOTO'")
    processing_time: float = Field(..., description="Total processing time in seconds")


class TempDirs(BaseModel):
    """Temporary directories used during processing"""
    input: str = Field(..., description="Temporary directory for input files")
    output: str = Field(..., description="Temporary directory for output files")
    preprocessing: str = Field(..., description="Temporary directory for preprocessing files")


class PipelineStages(BaseModel):
    """Status of each pipeline stage"""
    preprocessing: str = Field(..., description="Status of preprocessing stage")
    geometry: str = Field(..., description="Status of geometry normalization stage")
    colometry: str = Field(..., description="Status of colometry normalization stage")
    features: str = Field(..., description="Status of feature extraction stage")


class GeometryResponse(BaseModel):
    """Response model for geometry normalization endpoint"""
    status: str = Field(..., description="Processing status: 'success' or 'error'")
    input_filename: Optional[str] = Field(None, description="Name of the uploaded input file")
    output_files: OutputFiles = Field(..., description="Paths to generated output files")
    metadata: Metadata = Field(..., description="Processing metadata and applied transformations")
    qa_flags: Dict[str, Any] = Field(..., description="Quality assurance flags indicating potential issues")
    temp_dirs: TempDirs = Field(..., description="Temporary directories used during processing")


class AnalyzeResponse(BaseModel):
    """Response model for full document analysis endpoint"""
    status: str = Field(..., description="Processing status: 'success' or 'error'")
    input_filename: Optional[str] = Field(None, description="Name of the uploaded input file")
    output_files: OutputFiles = Field(..., description="Paths to generated output files")
    metadata: Metadata = Field(..., description="Processing metadata and applied transformations")
    qa_flags: Dict[str, Any] = Field(..., description="Quality assurance flags indicating potential issues")
    pipeline_stages: PipelineStages = Field(..., description="Status of each pipeline stage")
    temp_dirs: TempDirs = Field(..., description="Temporary directories used during processing")
    classification: Optional[Dict[str, Any]] = Field(None, description="Document classification result (if enabled)")


class NotImplementedResponse(BaseModel):
    """Response model for not yet implemented endpoints"""
    status: str = Field(..., description="Status indicating the endpoint is not implemented")
    message: str = Field(..., description="Message explaining that the feature is not yet available")


class StatusResponse(BaseModel):
    """Response model for pipeline status endpoint"""
    status: str = Field(..., description="Overall pipeline status")
    stages: List[str] = Field(..., description="List of available pipeline stages")


class OCRLineResponse(BaseModel):
    """Response model for a single OCR line"""
    text: str = Field(..., description="Le texte reconnu de la ligne")
    confidence: float = Field(..., description="Le score de confiance de la reconnaissance")
    bounding_box: List[float] = Field(
        ..., 
        min_length=4,
        max_length=4,
        description="Boîte englobante au format rectangle [x_min, y_min, x_max, y_max]"
    )


class FeaturesResponse(BaseModel):
    """Response model for feature extraction endpoint"""
    status: str = Field(..., description="Statut du traitement")
    filename: Optional[str] = Field(None, description="Nom du fichier d'entrée")
    line_count: int = Field(..., description="Nombre de lignes de texte extraites")
    lines: List[OCRLineResponse] = Field(..., description="Liste des lignes de texte extraites")
    processing_time: float = Field(..., description="Temps de traitement en secondes")


class ClassificationResponse(BaseModel):
    """Response model for document classification endpoint"""
    status: str = Field(..., description="Statut du traitement")
    filename: Optional[str] = Field(None, description="Nom du fichier d'entrée")
    document_type: Optional[str] = Field(None, description="Type de document détecté")
    confidence: float = Field(..., description="Confiance de la classification (0.0-1.0)")
    processing_time: float = Field(..., description="Temps de traitement en secondes")


class MultiPageClassificationResponse(BaseModel):
    """Response model for multi-page document classification endpoint"""
    status: str = Field(..., description="Statut du traitement: 'success', 'partial_success', ou 'error'")
    filename: Optional[str] = Field(None, description="Nom du fichier d'entrée")
    total_pages: int = Field(..., description="Nombre total de pages traitées")
    successful_pages: int = Field(..., description="Nombre de pages traitées avec succès")
    failed_pages: int = Field(..., description="Nombre de pages ayant échoué")
    results_by_page: List[PageClassificationResult] = Field(..., description="Résultats de classification par page")
    processing_time: float = Field(..., description="Temps de traitement en secondes")
    message: Optional[str] = Field(None, description="Message décrivant le statut du traitement")


class TaskResponse(BaseModel):
    """Response model for task creation"""
    task_id: str = Field(..., description="Unique task identifier")
    status: str = Field(..., description="Current task status")
    message: str = Field(..., description="Human-readable message")


class TaskStatusResponse(BaseModel):
    """Response model for task status query"""
    task_id: str = Field(..., description="Task identifier")
    status: str = Field(..., description="Current task status")
    created_at: Optional[str] = Field(None, description="Task creation timestamp")
    started_at: Optional[str] = Field(None, description="Task start timestamp")
    completed_at: Optional[str] = Field(None, description="Task completion timestamp")


class WarmupResponse(BaseModel):
    """Response model for worker warm-up endpoint"""
    status: str = Field(..., description="Warm-up status")
    message: str = Field(..., description="Human-readable message")
    task_count: Optional[int] = Field(None, description="Number of warm-up tasks sent")


class OCRHealthResponse(BaseModel):
    """Response model for OCR service health check"""
    status: str = Field(..., description="Health check status: 'healthy' or 'unhealthy'")
    engine_initialized: bool = Field(..., description="Whether the OCR engine is initialized")
    worker_pid: Optional[int] = Field(None, description="Process ID of the worker that responded")
    message: str = Field(..., description="Message describing the health check result")
    response_time_ms: Optional[float] = Field(None, description="Response time in milliseconds")


# Helper function for background task execution
def execute_processing_task_sync(
    task_id: str,
    file_path: str,
    filename: str,
    temp_dirs: Dict[str, str],
    preprocessing_normalizer,
    geometry_normalizer,
    request_id: Optional[str] = None,
    is_full_analysis: bool = False
) -> None:
    """
    Execute processing task synchronously (to be run in background).
    
    Args:
        task_id: Task ID
        file_path: Path to input file
        filename: Original filename
        temp_dirs: Temporary directories
        preprocessing_normalizer: Preprocessing normalizer (injected)
        geometry_normalizer: Geometry normalizer (injected)
        request_id: Request ID for correlation (optional, will use context if not provided)
        is_full_analysis: Whether to run full analysis or just geometry
    """
    # Importer ici pour éviter les imports circulaires
    from src.utils.logger import set_request_id
    
    # Définir le request_id dans le contexte pour cette tâche
    if request_id:
        set_request_id(request_id)
    
    try:
        # Create service with injected dependencies
        service = ProcessingService(
            preprocessing_normalizer=preprocessing_normalizer,
            geometry_normalizer=geometry_normalizer
        )
        
        # Update task status to processing
        task_manager.update_task_status(task_id, TaskStatus.PROCESSING)
        logger.info(f"Début du traitement de la tâche {task_id}")
        
        # Run processing
        if is_full_analysis:
            result = service.process_full_analysis(
                file_path,
                filename,
                temp_dirs
            )
        else:
            result = service.process_geometry(
                file_path,
                filename,
                temp_dirs
            )
        
        # Update task with result
        task_manager.update_task_status(task_id, TaskStatus.COMPLETED, result=result)
        logger.info(f"Tâche {task_id} complétée avec succès")
        
        # Schedule cleanup after task completion
        # Note: In production, this should be handled by a proper task queue system
        cleanup_temp_files(temp_dirs)
        
    except Exception as e:
        error_msg = str(e)
        logger.error(f"Erreur lors de l'exécution de la tâche {task_id}: {e}", exc_info=True)
        task_manager.update_task_status(task_id, TaskStatus.FAILED, error=error_msg)
        
        # Cleanup on error as well
        cleanup_temp_files(temp_dirs)


def cleanup_temp_files(temp_dirs: Dict[str, str]) -> None:
    """
    Clean up temporary files after response is sent.
    This function is designed to be called as a background task.
    
    Args:
        temp_dirs: Dictionary with temporary directory paths
    """
    # Create a temporary service instance for cleanup (no dependencies needed)
    service = ProcessingService()
    service.cleanup_temp_directories(temp_dirs)
    logger.info("Nettoyage des fichiers temporaires terminé")


@router.post(
    "/analyze",
    response_model=TaskResponse,
    status_code=202,
    summary="Complete Document Analysis (Async)",
    description="""
    Submits a document for complete analysis. Returns immediately with a task_id.
    Use GET /results/{task_id} to check the status and retrieve results.
    
    **Current Implementation:**
    - Preprocessing: Image enhancement, contrast improvement, and capture type classification (SCAN vs PHOTO)
    - Geometry Normalization: Intelligent cropping, deskew (angle correction), and orientation correction
    
    **Future Stages (Not Yet Implemented):**
    - Colometry Normalization: Column structure normalization
    - Feature Extraction: OCR text extraction and checkbox detection
    
    **Input:**
    - Accepts image files (PNG, JPG, JPEG, TIFF, BMP) or PDF files
    - Maximum file size depends on server configuration
    
    **Response:**
    - Returns 202 Accepted with a task_id
    - Processing happens asynchronously in the background
    - Use GET /results/{task_id} to check status and get results
    
    **Error Handling:**
    - Returns 400 for invalid file formats or empty files
    - Returns 500 for server errors
    """
)
async def analyze_document(
    background_tasks: BackgroundTasks,
    preprocessing_normalizer: Annotated[PreprocessingNormalizer, Depends(get_preprocessing_normalizer)],
    geometry_normalizer: Annotated[GeometryNormalizer, Depends(get_geometry_normalizer)],
    file: UploadFile = File(...)
) -> TaskResponse:
    """
    Submit document for complete analysis (asynchronous).
    """
    try:
        # Create service with injected dependencies
        service = ProcessingService(
            preprocessing_normalizer=preprocessing_normalizer,
            geometry_normalizer=geometry_normalizer
        )
        
        # 1. Validate file
        file_content = await file.read()
        file_extension, error = service.validate_file(file.filename, len(file_content))
        if error:
            raise HTTPException(status_code=400, detail=error)
        
        # 2. Create task
        task_id = task_manager.create_task(file.filename)
        
        # 3. Create temporary directories
        temp_dirs = service.create_temp_directories(prefix="analyze_")
        
        # 4. Save uploaded file
        temp_file_path = service.save_uploaded_file(
            file_content,
            file.filename or f"upload{file_extension}",
            temp_dirs['input']
        )
        
        # 5. Schedule background task for processing
        # Note: For production, consider using Dramatiq for better task management
        # Récupérer le request_id pour le propager à la tâche asynchrone
        current_request_id = get_request_id()
        background_tasks.add_task(
            execute_processing_task_sync,
            task_id=task_id,
            file_path=temp_file_path,
            filename=file.filename,
            temp_dirs=temp_dirs,
            preprocessing_normalizer=preprocessing_normalizer,
            geometry_normalizer=geometry_normalizer,
            request_id=current_request_id,
            is_full_analysis=True
        )
        
        # 6. Schedule cleanup after response is sent (with delay to allow task to complete)
        # Note: In production, cleanup should happen after task completion, not immediately
        # For now, we keep files until task is completed
        # background_tasks.add_task(cleanup_temp_files, temp_dirs)
        
        logger.info(f"Tâche d'analyse créée: {task_id} pour le fichier {file.filename}")
        
        return TaskResponse(
            task_id=task_id,
            status=TaskStatus.PENDING.value,
            message="Document analysis task created. Use GET /results/{task_id} to check status."
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erreur inattendue lors de la création de la tâche: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Erreur interne lors de la création de la tâche: {str(e)}"
        )


@router.post(
    "/pipeline/colometry",
    response_model=NotImplementedResponse,
    status_code=501,
    summary="Column Structure Normalization",
    description="""
    Normalizes the column structure of a document.
    
    **Status:** Not yet implemented
    
    **Planned Functionality:**
    This endpoint will normalize the column layout of documents, ensuring consistent column widths,
    detecting multi-column layouts, and standardizing column boundaries.
    
    **Future Parameters:**
    - Expected number of columns
    - Minimum column width thresholds
    - Column spacing normalization
    
    **Note:** This feature is planned for future implementation. Check the pipeline status endpoint
    for current implementation status.
    """
)
async def pipeline_colometry(file: UploadFile = File(...)) -> NotImplementedResponse:
    """
    Column structure normalization endpoint (not yet implemented).
    """
    return NotImplementedResponse(
        status="not_implemented",
        message="Column structure normalization - Not yet implemented"
    )


@router.post(
    "/pipeline/geometry",
    response_model=None,  # Désactiver la génération automatique car on retourne Union[GeometryResponse, JSONResponse]
    summary="Document Geometry Normalization",
    description="""
    Accepts an image or PDF file, applies preprocessing (enhancement, contrast improvement, classification)
    then performs geometry normalization (crop, deskew, rotation).
    
    **By default, this endpoint is asynchronous** and returns immediately with a task_id.
    Use GET /results/{task_id} to check the status and retrieve results.
    
    For backward compatibility, you can use `?sync=true` to get synchronous behavior
    (returns results directly, but may timeout for large documents).
    
    **Processing Steps:**
    
    1. **Preprocessing:**
       - Image enhancement and contrast improvement
       - Capture type classification (SCAN vs PHOTO)
       - White level analysis to determine document type
    
    2. **Geometry Normalization:**
       - **Intelligent Cropping:** Automatically crops the document from background using doctr
       - **Deskew (Angle Correction):** Corrects document rotation/angle skew using Hough line detection
       - **Orientation Correction:** Detects and corrects document orientation (0°, 90°, 180°, 270°) using ONNX model
    
    3. **Quality Assurance:**
       - Generates QA flags for potential issues (low contrast, overcrop risk, small resolution, etc.)
       - Validates minimum quality standards
    
    **Input:**
    - Accepts image files (PNG, JPG, JPEG, TIFF, BMP) or PDF files
    - PDFs are converted to images at 300 DPI
    
    **Query Parameters:**
    - **sync** (optional, default: false): If true, returns results synchronously (for backward compatibility).
      If false (default), returns a task_id immediately and processes asynchronously.
    
    **Output (Async mode, default):**
    - Returns 202 Accepted with a task_id
    - Use GET /results/{task_id} to check status and get results
    
    **Output (Sync mode, sync=true):**
    - **transformed:** Path to the final processed image after all transformations
    - **original:** Path to the original image (if saved)
    - **transform_file:** JSON file containing all applied transformations
    - **qa_file:** JSON file containing quality assurance flags
    
    **Metadata Includes:**
    - Which transformations were applied (crop, deskew, rotation)
    - Detected orientation angle
    - Capture type (SCAN or PHOTO)
    - Processing time
    
    **Error Handling:**
    - Returns 400 for invalid file formats or empty files
    - Returns 500 for processing errors (preprocessing, geometry, model loading, etc.)
    """
)
async def pipeline_geometry(
    background_tasks: BackgroundTasks,
    preprocessing_normalizer: Annotated[PreprocessingNormalizer, Depends(get_preprocessing_normalizer)],
    geometry_normalizer: Annotated[GeometryNormalizer, Depends(get_geometry_normalizer)],
    file: UploadFile = File(...),
    sync: bool = False
) -> Union[GeometryResponse, JSONResponse]:
    """
    Process document geometry normalization.
    
    By default, processes asynchronously and returns a task_id.
    Use sync=true for synchronous processing (backward compatibility).
    """
    # Create service with injected dependencies
    service = ProcessingService(
        preprocessing_normalizer=preprocessing_normalizer,
        geometry_normalizer=geometry_normalizer
    )
    
    temp_dirs = None
    
    try:
        # 1. Validate file
        file_content = await file.read()
        file_extension, error = service.validate_file(file.filename, len(file_content))
        if error:
            raise HTTPException(status_code=400, detail=error)
        
        # 2. Create temporary directories
        temp_dirs = service.create_temp_directories(prefix="geometry_")
        
        # 3. Save uploaded file
        temp_file_path = service.save_uploaded_file(
            file_content,
            file.filename or f"upload{file_extension}",
            temp_dirs['input']
        )
        
        # 4. Process document (async or sync based on parameter)
        if sync:
            # Synchronous mode (backward compatibility)
            logger.info(f"Traitement géométrique synchrone pour {file.filename}")
            response_data = service.process_geometry(
                temp_file_path,
                file.filename,
                temp_dirs
            )
            
            # Schedule cleanup after response is sent
            background_tasks.add_task(cleanup_temp_files, temp_dirs)
            
            return response_data
        else:
            # Asynchronous mode (default)
            # Create task
            task_id = task_manager.create_task(file.filename)
            
            # Schedule background task for processing
            # Récupérer le request_id pour le propager à la tâche asynchrone
            current_request_id = get_request_id()
            background_tasks.add_task(
                execute_processing_task_sync,
                task_id=task_id,
                file_path=temp_file_path,
                filename=file.filename,
                temp_dirs=temp_dirs,
                preprocessing_normalizer=preprocessing_normalizer,
                geometry_normalizer=geometry_normalizer,
                request_id=current_request_id,
                is_full_analysis=False  # Only geometry processing
            )
            
            logger.info(f"Tâche de géométrie créée: {task_id} pour le fichier {file.filename}")
            
            task_response = TaskResponse(
                task_id=task_id,
                status=TaskStatus.PENDING.value,
                message="Geometry normalization task created. Use GET /results/{task_id} to check status."
            )
            
            # Return 202 Accepted for async requests
            return JSONResponse(
                status_code=202,
                content=task_response.model_dump()
            )
        
    except HTTPException:
        raise
    except (PreprocessingError, GeometryError, ModelLoadingError, ImageProcessingError) as e:
        logger.error(f"Erreur du pipeline: {e}", exc_info=True)
        # Cleanup on error
        if temp_dirs:
            cleanup_temp_files(temp_dirs)
        raise HTTPException(
            status_code=500,
            detail=f"Erreur lors du traitement: {str(e)}"
        )
    except ValueError as e:
        logger.error(f"Erreur de validation: {e}", exc_info=True)
        # Cleanup on error
        if temp_dirs:
            cleanup_temp_files(temp_dirs)
        raise HTTPException(status_code=400, detail=str(e))
    except PipelineError as e:
        logger.error(f"Erreur du pipeline: {e}", exc_info=True)
        # Cleanup on error
        if temp_dirs:
            cleanup_temp_files(temp_dirs)
        raise HTTPException(
            status_code=500,
            detail=f"Erreur du pipeline: {str(e)}"
        )
    except Exception as e:
        logger.error(f"Erreur inattendue lors du traitement géométrique: {e}", exc_info=True)
        # Cleanup on error
        if temp_dirs:
            cleanup_temp_files(temp_dirs)
        raise HTTPException(
            status_code=500,
            detail=f"Erreur interne lors du traitement: {str(e)}"
        )


def pdf_buffer_to_images_with_error_handling(pdf_buffer: bytes, dpi: int = 300) -> List[np.ndarray]:
    """
    Wrapper autour de pdf_buffer_to_images pour convertir les exceptions en HTTPException.
    
    Args:
        pdf_buffer: Contenu du PDF en bytes
        dpi: Résolution DPI pour la conversion
        
    Returns:
        List[np.ndarray]: Liste d'images numpy (une par page)
        
    Raises:
        HTTPException: Si la conversion échoue
    """
    try:
        # Utiliser min_dpi depuis la config (défaut: 300 si non disponible)
        from src.utils.config_loader import get_config
        try:
            pdf_config = get_config().pdf
            min_dpi = pdf_config.min_dpi
        except:
            min_dpi = 300  # Fallback si la config n'est pas disponible
        
        return pdf_buffer_to_images(pdf_buffer, dpi=dpi, min_dpi=min_dpi)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except RuntimeError as e:
        raise HTTPException(status_code=500, detail=str(e))
    except Exception as e:
        raise HTTPException(
            status_code=400,
            detail=f"Impossible de convertir le PDF en image: {str(e)}"
        )


@router.post(
    "/pipeline/features",
    response_model=FeaturesResponse,
    summary="Feature Extraction (OCR)",
    description="""
    Extrait le texte d'un document image ou PDF, ligne par ligne, en utilisant PaddleOCR.
    
    **Processus :**
    1. Valide et charge le fichier uploadé.
    2. Si c'est un PDF, convertit la première page en image.
    3. Appelle le moteur OCR pour extraire chaque ligne de texte.
    4. Retourne une liste structurée de lignes avec texte, confiance et coordonnées.
    
    Cette étape est optimisée pour la vitesse et sert de base pour la classification
    de documents et la localisation d'ancres.
    """
)
async def pipeline_features(
    feature_extractor: Annotated[FeatureExtractor, Depends(get_feature_extractor)],
    file: UploadFile = File(...)
) -> FeaturesResponse:
    """
    Extrait les features (OCR) d'un document.
    """
    start_time = time.time()
    
    try:
        # Lire le contenu du fichier
        file_content = await file.read()
        if not file_content:
            raise HTTPException(status_code=400, detail="Le fichier est vide.")
        
        # Convertir en image NumPy
        image_np: np.ndarray
        
        if is_pdf(file.filename or ""):
            # Utiliser PyMuPDF pour convertir le PDF en mémoire (première page seulement)
            images_np = pdf_buffer_to_images_with_error_handling(file_content, dpi=300)
            image_np = images_np[0] if images_np else None
            if image_np is None:
                raise HTTPException(status_code=400, detail="Le PDF est vide.")
        else:
            # Convertir l'image directement depuis le buffer mémoire
            image_np = cv2.imdecode(np.frombuffer(file_content, np.uint8), cv2.IMREAD_COLOR)
            if image_np is None:
                raise HTTPException(status_code=400, detail="Format d'image invalide ou corrompu.")
        
        # Appeler notre FeatureExtractor
        extracted_lines = feature_extractor.extract_ocr(image_np)
        
        processing_time = time.time() - start_time
        
        # Convertir les dictionnaires en OCRLineResponse
        ocr_lines = [
            OCRLineResponse(**line) for line in extracted_lines
        ]
        
        return FeaturesResponse(
            status="success",
            filename=file.filename,
            line_count=len(ocr_lines),
            lines=ocr_lines,
            processing_time=processing_time
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erreur lors de l'extraction de features : {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Erreur interne du serveur : {str(e)}"
        )


@router.post(
    "/classify",
    # La réponse est maintenant un TaskResponse, pas MultiPageClassificationResponse
    response_model=TaskResponse,
    status_code=202,  # 202 Accepted, car le traitement est déféré
    summary="Submit a document for classification (Asynchronous)",
    description="""
    Chef d'Orchestre léger : Soumet un document pour classification. Répond immédiatement avec un task_id.
    Utilisez GET /api/v1/classify/results/{task_id} pour obtenir le résultat.
    
    **Processus :**
    1. Reçoit le fichier
    2. Le découpe en images (s'il s'agit d'un PDF)
    3. Sauvegarde chaque image via le StorageService
    4. Pour chaque page, crée une tâche unique et indépendante process_page_task avec l'identifiant de l'image et l'ID du document global
    5. Retourne immédiatement un task_id global
    
    Chaque tâche process_page_task est indépendante et effectue :
    - Chargement de l'image depuis le stockage
    - Extraction OCR via le microservice OCR
    - Classification du document
    
    **Input:**
    - Accepts image files (PNG, JPG, JPEG, TIFF, BMP) or PDF files
    
    **Output:**
    - Retourne immédiatement un task_id avec le statut "pending"
    - Utilisez GET /api/v1/classify/results/{task_id} pour obtenir les résultats
    
    **Note:** La classification doit être activée dans config.yaml (classification.enabled: true)
    """
)
async def classify_document(file: UploadFile = File(...)) -> TaskResponse:
    """
    Chef d'Orchestre léger : Reçoit le fichier, le découpe en images, sauvegarde chaque image,
    crée une tâche process_page_task pour chaque page, et retourne immédiatement un task_id global.
    
    L'endpoint ne fait plus l'OCR - chaque tâche process_page_task est indépendante et fait
    son propre OCR + classification.
    """
    from src.utils.storage import get_storage
    from src.workers import process_page_task, aggregate_classification_results
    import uuid
    
    # Log au début du traitement
    filename = file.filename or "unknown"
    current_request_id = get_request_id()
    logger.info(f"Début de la classification pour '{filename}'. request_id: {current_request_id}.")
    
    file_content = await file.read()
    if not file_content:
        raise HTTPException(status_code=400, detail="Le fichier est vide.")

    # === ÉTAPE 1 : Découper le fichier en images (si PDF) ===
    storage = get_storage()
    
    images_np: List[np.ndarray]
    if is_pdf(filename):
        try:
            from src.utils.config_loader import get_config
            pdf_config = get_config().pdf
            min_dpi = pdf_config.min_dpi
        except:
            min_dpi = 300
        try:
            images_np = pdf_buffer_to_images(file_content, dpi=300, min_dpi=min_dpi)
        except Exception as e:
            error_msg = f"Échec de la conversion PDF en images: {str(e)}"
            logger.error(error_msg)
            raise HTTPException(status_code=500, detail=error_msg)
    else:
        img_array = np.frombuffer(file_content, np.uint8)
        img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)
        if img is None:
            raise HTTPException(status_code=400, detail="Format d'image invalide ou corrompu")
        images_np = [img]
    
    if not images_np:
        raise HTTPException(status_code=400, detail="Aucune image n'a pu être extraite du fichier")
    
    num_pages = len(images_np)
    document_id = f"doc_{uuid.uuid4().hex[:12]}"
    
    # Log après la préparation des pages
    logger.info(f"Document découpé en {num_pages} page(s). document_id: {document_id}. request_id: {current_request_id}.")
    
    # === ÉTAPE 2 : Sauvegarder chaque image via StorageService ===
    image_identifiers = []
    for page_index, image_np in enumerate(images_np):
        image_identifier = storage.save_image(image_np, page_index=page_index, task_id=document_id)
        image_identifiers.append(image_identifier)
    
    # Stocker les identifiants d'images et le filename dans Redis pour le nettoyage ultérieur et la récupération
    try:
        import json
        from src.workers import process_page_task
        # Utiliser le client Redis du broker Dramatiq
        redis_client = process_page_task.broker.client
        image_identifiers_key = f"document:{document_id}:image_identifiers"
        redis_client.setex(image_identifiers_key, 86400, json.dumps(image_identifiers))  # 24 heures
        
        # Stocker aussi le filename pour la récupération par le Janitor
        filename_key = f"document:{document_id}:filename"
        redis_client.setex(filename_key, 86400, filename)  # 24 heures
    except Exception as e:
        logger.warning(f"Impossible de stocker les métadonnées dans Redis: {e}")
    
    logger.info(
        f"Document '{filename}' découpé en {num_pages} page(s) et sauvegardé",
        extra={
            "metrics": {
                "filename": filename,
                "num_pages": num_pages,
                "document_id": document_id,
                "stage": "document_split_and_saved"
            }
        }
    )
    
    # === ÉTAPE 3 : Créer une tâche process_page_task pour chaque page ===
    page_messages = []
    
    for page_index, image_identifier in enumerate(image_identifiers):
        if current_request_id:
            message = process_page_task.send(
                image_identifier,
                page_index,
                document_id,
                options={"message_metadata": {"request_id": current_request_id}}
            )
        else:
            message = process_page_task.send(image_identifier, page_index, document_id)
        page_messages.append(message)
    
    # === ÉTAPE 4 : Créer la tâche de finalisation (Le Contremaître) ===
    # Cette tâche s'exécutera avec un délai initial (eta) pour laisser le temps aux pages de se traiter
    from src.workers import finalize_document_task
    from datetime import datetime, timedelta
    import time
    
    # Délai initial : 5 secondes pour laisser le temps aux premières pages de commencer
    initial_delay_seconds = 5
    # Convertir datetime en timestamp Unix (millisecondes) pour Dramatiq
    eta_datetime = datetime.now() + timedelta(seconds=initial_delay_seconds)
    eta_timestamp_ms = int(eta_datetime.timestamp() * 1000)  # Dramatiq attend des millisecondes
    
    if current_request_id:
        finalize_message = finalize_document_task.send(
            document_id,
            filename,
            num_pages,
            options={
                "eta": eta_timestamp_ms,
                "message_metadata": {"request_id": current_request_id}
            }
        )
    else:
        finalize_message = finalize_document_task.send(
            document_id,
            filename,
            num_pages,
            options={"eta": eta_timestamp_ms}
        )
    
    # L'ID de la tâche de finalisation est notre task_id global
    global_task_id = finalize_message.message_id
    
    # Log avant de retourner la réponse
    logger.info(f"Tâche de finalisation créée avec task_id: {global_task_id}. Réponse 202 envoyée au client. document_id: {document_id}. request_id: {current_request_id}.")
    
    logger.info(
        f"Tâche globale créée: {global_task_id} pour le document '{document_id}' ({num_pages} page(s))",
        extra={
            "metrics": {
                "task_id": global_task_id,
                "document_id": document_id,
                "filename": filename,
                "num_pages": num_pages,
                "request_id": current_request_id
            }
        }
    )

    return TaskResponse(
        task_id=global_task_id,
        status="pending",
        message="La tâche de classification a été créée. Vérifiez le statut avec l'endpoint de résultats."
    )


@router.get(
    "/classify/results/{task_id}",
    # Utilisez un modèle de réponse flexible
    response_model=Dict[str, Any],
    summary="Get classification task status and result",
    description="""
    Récupère le statut et le résultat d'une tâche de classification.
    
    **Response Status:**
    - **pending:** Tâche créée mais pas encore terminée
    - **completed:** Tâche terminée avec succès, résultats disponibles
    - **failed:** Tâche échouée, message d'erreur disponible
    
    **Response Fields:**
    - **task_id:** Identifiant de la tâche
    - **status:** Statut actuel de la tâche
    - **result:** Résultats de classification (si completed)
    - **error:** Message d'erreur (si failed)
    """
)
def get_classification_result(task_id: str):
    """
    Récupère le statut et le résultat d'une tâche de classification.
    """
    try:
        # Récupérer le broker et trouver le middleware Results
        # Utiliser finalize_document_task car c'est maintenant la tâche principale
        from src.workers import finalize_document_task
        broker = finalize_document_task.broker
        results_middleware = None
        
        # Chercher le middleware Results dans les middlewares du broker
        for middleware in broker.middleware:
            if isinstance(middleware, Results):
                results_middleware = middleware
                break
        
        if results_middleware is None:
            return {"task_id": task_id, "status": "error", "error": "Backend de résultats non configuré"}
        
        # Créer un message avec le task_id
        message = finalize_document_task.message().copy(message_id=task_id)
        
        # Obtenir le résultat depuis le backend (peut lever ResultMissing si pas encore prêt)
        backend = results_middleware.backend
        result = backend.get_result(message, block=False)
        
        return {
            "task_id": task_id,
            "status": "completed",
            "result": result
        }

    except ResultMissing:
        # ResultMissing signifie que le résultat n'est pas encore disponible
        # Cela peut signifier soit que la tâche est en cours, soit qu'elle n'existe pas
        # 
        # IMPORTANT: On ne retourne 404 que si on peut confirmer avec CERTITUDE
        # que la tâche n'existe vraiment nulle part. Sinon, on retourne "pending"
        # pour éviter les faux négatifs (surtout si la tâche vient d'être créée
        # et n'est pas encore complètement dans Redis).
        try:
            from dramatiq.brokers.redis import RedisBroker
            from dramatiq.results.backends import RedisBackend
            
            if isinstance(broker, RedisBroker) and isinstance(backend, RedisBackend):
                redis_client = broker.client
                
                # Vérifier toutes les clés Redis possibles qui pourraient contenir ce message_id
                # 1. Résultat complet
                result_key = f"dramatiq:results:{task_id}"
                # 2. Résultat d'erreur
                error_key = f"dramatiq:results:{task_id}:error"
                
                # Vérifier si une clé de résultat existe (même vide ou en erreur)
                result_exists = redis_client.exists(result_key) or redis_client.exists(error_key)
                
                # Vérifier si le message existe dans les queues Dramatiq
                # Dramatiq utilise le format: dramatiq:queue:{queue_name}
                queue_name = classify_document_task.queue_name or "default"
                full_queue_name = f"dramatiq:queue:{queue_name}"
                
                message_exists = False
                try:
                    # Vérifier si la queue existe et contient des messages
                    queue_length = redis_client.llen(full_queue_name)
                    if queue_length > 0:
                        # Chercher le message dans la queue (vérifier les premiers messages)
                        # On limite à 100 messages pour éviter de scanner toute la queue
                        messages = redis_client.lrange(full_queue_name, 0, min(100, queue_length) - 1)
                        for msg_data in messages:
                            try:
                                import json
                                msg = json.loads(msg_data.decode('utf-8'))
                                if msg.get('message_id') == task_id:
                                    message_exists = True
                                    break
                            except:
                                pass
                except Exception:
                    # Si on ne peut pas vérifier la queue, on assume que le message pourrait exister
                    pass
                
                # Vérifier aussi dans la queue "default" au cas où
                if not message_exists:
                    try:
                        default_queue_name = "dramatiq:queue:default"
                        queue_length = redis_client.llen(default_queue_name)
                        if queue_length > 0:
                            messages = redis_client.lrange(default_queue_name, 0, min(100, queue_length) - 1)
                            for msg_data in messages:
                                try:
                                    import json
                                    msg = json.loads(msg_data.decode('utf-8'))
                                    if msg.get('message_id') == task_id:
                                        message_exists = True
                                        break
                                except:
                                    pass
                    except Exception:
                        pass
                
                # On ne retourne 404 QUE si on peut confirmer avec certitude que:
                # 1. Le résultat n'existe pas
                # 2. Le message n'est pas dans les queues
                # 3. ET que le message_id a un format valide (pour éviter les faux positifs)
                # 
                # Note: On ne vérifie PAS avec redis.keys() car:
                # - C'est très lent sur de grandes bases
                # - Peut être désactivé en production
                # - Le message peut être en cours de traitement et ne pas être dans une queue
                if not result_exists and not message_exists:
                    # Vérifier que le task_id a un format UUID valide
                    # Si ce n'est pas un UUID valide, c'est probablement une erreur
                    import uuid
                    try:
                        uuid.UUID(task_id)
                        # Format valide, mais message introuvable - retourner 404
                        # MAIS seulement après un délai raisonnable (on ne vérifie pas ici)
                        # Pour l'instant, on retourne "pending" pour être sûr
                        # TODO: Implémenter un cache des task_ids créés récemment
                    except ValueError:
                        # Format invalide, retourner 404
                        raise HTTPException(
                            status_code=404,
                            detail=f"Task {task_id} not found (invalid task_id format)"
                        )
        except HTTPException:
            raise
        except Exception:
            # Si on ne peut pas vérifier (erreur de connexion Redis, etc.),
            # on assume que la tâche est pending pour éviter les faux négatifs
            # C'est le comportement le plus sûr
            pass
        
        # Par défaut, si ResultMissing est levé, on retourne "pending"
        # C'est le comportement le plus sûr car:
        # 1. La tâche peut venir d'être créée et ne pas être encore dans Redis
        # 2. La tâche peut être en cours de traitement et ne pas être dans une queue
        # 3. Mieux vaut retourner "pending" que 404 pour une tâche qui existe
        return {"task_id": task_id, "status": "pending"}
    except HTTPException:
        raise
    except Exception as e:
        # Gérer le cas où la tâche a échoué (Dramatiq stocke l'exception)
        try:
            from src.workers import finalize_document_task
            broker = finalize_document_task.broker
            results_middleware = None
            for middleware in broker.middleware:
                if isinstance(middleware, Results):
                    results_middleware = middleware
                    break
            
            if results_middleware:
                backend = results_middleware.backend
                message = finalize_document_task.message().copy(message_id=task_id)
                # Tenter de récupérer le résultat lèvera l'exception originale
                backend.get_result(message)
        except Exception as task_error:
             return {"task_id": task_id, "status": "failed", "error": str(task_error)}
        
        # Fallback
        return {"task_id": task_id, "status": "failed", "error": f"Erreur inconnue: {str(e)}"}


@router.get(
    "/results/{task_id}",
    response_model=TaskStatusResponse,
    summary="Get Task Status and Results",
    description="""
    Retrieve the status and results of an asynchronous processing task.
    
    **Use Cases:**
    - Check if a task is still processing
    - Retrieve results when task is completed
    - Get error information if task failed
    
    **Response Status:**
    - **pending:** Task created but not yet started
    - **processing:** Task is currently being processed
    - **completed:** Task completed successfully, results available
    - **failed:** Task failed, error message available
    
    **Response Fields:**
    - **task_id:** Unique task identifier
    - **status:** Current task status
    - **created_at:** When the task was created
    - **started_at:** When processing started (if started)
    - **completed_at:** When processing completed (if completed)
    - **result:** Complete processing results (if completed)
    - **error:** Error message (if failed)
    - **filename:** Original filename
    
    **Error Handling:**
    - Returns 404 if task_id is not found
    """
)
async def get_task_results(task_id: str) -> TaskStatusResponse:
    """
    Get the status and results of an asynchronous processing task.
    """
    task = task_manager.get_task(task_id)
    
    if not task:
        raise HTTPException(
            status_code=404,
            detail=f"Task {task_id} not found"
        )
    
    return TaskStatusResponse(**task.to_dict())


@router.get(
    "/pipeline/status",
    response_model=StatusResponse,
    summary="Pipeline Status",
    description="""
    Returns the current status of the document analysis pipeline.
    
    **Use Cases:**
    - Check which pipeline stages are available and implemented
    - Verify API health and readiness
    - Determine which endpoints are functional
    
    **Response:**
    - **status:** Overall pipeline status (e.g., "ready", "maintenance")
    - **stages:** List of available pipeline stages
    
    **Available Stages:**
    - **preprocessing:** Image enhancement and capture type classification (implemented)
    - **geometry:** Geometry normalization - crop, deskew, rotation (implemented)
    - **colometry:** Column structure normalization (not yet implemented)
    - **features:** Feature extraction - OCR and checkbox detection (not yet implemented)
    
    This endpoint does not require any input and can be used for health checks.
    """
)
async def pipeline_status() -> StatusResponse:
    """
    Returns the current status of the document analysis pipeline.
    """
    return StatusResponse(
        status="ready",
        stages=["preprocessing", "geometry", "colometry", "features"]
    )


@router.post(
    "/warmup",
    response_model=WarmupResponse,
    status_code=202,
    summary="Warm-up Workers",
    description="""
    Envoie une tâche de "warm-up" aux workers pour pré-initialiser les modèles.
    
    Cette tâche force l'initialisation des modèles lourds (PaddleOCR, PyTorch, etc.)
    dans les workers Dramatiq. C'est particulièrement utile en production avec
    auto-scaling pour "chauffer" les nouveaux workers avant qu'ils ne reçoivent
    de vraies tâches de traitement.
    
    **Use Cases:**
    - Pré-initialiser les workers après un redémarrage
    - "Chauffer" les nouveaux workers dans un environnement auto-scaling
    - Vérifier que les workers sont prêts à traiter des tâches
    
    **How it works:**
    - Envoie une tâche légère qui ne fait rien d'utile mais force l'initialisation
    - Les modèles sont chargés en mémoire dans le processus worker
    - Les futures tâches de traitement seront plus rapides car les modèles sont déjà chargés
    
    **Query Parameters:**
    - **count** (optional, default: 1): Nombre de tâches de warm-up à envoyer.
      Utile pour chauffer plusieurs workers en parallèle.
    
    **Response:**
    - Returns 202 Accepted avec le nombre de tâches envoyées
    - Les tâches sont traitées de manière asynchrone
    - Aucun résultat n'est retourné (les tâches ne stockent pas de résultats)
    
    **Note:** Cette tâche est légère et ne consomme pas de ressources significatives.
    Elle peut être appelée régulièrement sans impact sur les performances.
    """
)
async def warmup_workers(count: int = 1) -> WarmupResponse:
    """
    Envoie une ou plusieurs tâches de warm-up aux workers pour pré-initialiser les modèles.
    
    Args:
        count: Nombre de tâches de warm-up à envoyer (défaut: 1)
    
    Returns:
        WarmupResponse: Statut et nombre de tâches envoyées
    """
    if count < 1:
        raise HTTPException(
            status_code=400,
            detail="Le nombre de tâches doit être supérieur ou égal à 1"
        )
    
    if count > 100:
        raise HTTPException(
            status_code=400,
            detail="Le nombre de tâches ne peut pas dépasser 100 pour éviter la surcharge"
        )
    
    # Récupérer le request_id pour le propager dans les métadonnées des tâches
    current_request_id = get_request_id()
    
    # Envoyer les tâches de warm-up
    # Propager le request_id dans les métadonnées pour le traçage de bout en bout
    tasks_sent = 0
    for _ in range(count):
        try:
            if current_request_id:
                warmup_worker_task.send(
                    options={"message_metadata": {"request_id": current_request_id}}
                )
            else:
                warmup_worker_task.send()
            tasks_sent += 1
        except Exception as e:
            logger.error(f"Erreur lors de l'envoi d'une tâche de warm-up: {e}", exc_info=True)
            # Continuer à envoyer les autres tâches même si une échoue
    
    logger.info(
        f"Tâches de warm-up envoyées: {tasks_sent}/{count}",
        extra={
            "metrics": {
                "warmup_tasks_requested": count,
                "warmup_tasks_sent": tasks_sent,
                "status": "sent"
            }
        }
    )
    
    if tasks_sent == 0:
        raise HTTPException(
            status_code=500,
            detail="Impossible d'envoyer les tâches de warm-up"
        )
    
    message = f"{tasks_sent} tâche(s) de warm-up envoyée(s) aux workers"
    if tasks_sent < count:
        message += f" ({count - tasks_sent} échec(s))"
    
    return WarmupResponse(
        status="sent",
        message=message,
        task_count=tasks_sent
    )


@router.get(
    "/ocr/health",
    response_model=OCRHealthResponse,
    summary="OCR Service Health Check",
    description=""""
    Vérifie la santé du microservice OCR et s'assure que le moteur PaddleOCR est initialisé.
    
    Cet endpoint envoie une tâche de warm-up au microservice OCR et attend le résultat
    pour valider que le service est opérationnel. Si le service répond dans le délai
    imparti et que le moteur OCR est initialisé, le service est considéré comme sain.
    
    **Use Cases:**
    - Vérifier que le microservice OCR est démarré et prêt
    - Valider que les modèles PaddleOCR sont chargés
    - Monitoring de la santé du service OCR
    - Health checks pour les load balancers ou orchestrateurs
    
    **How it works:**
    - Envoie une tâche de warm-up au microservice OCR via la queue ocr-queue
    - Attend le résultat avec un timeout de 30 secondes
    - Vérifie que le moteur OCR est initialisé (engine_initialized: true)
    - Retourne le statut de santé du service
    
    **Response:**
    - **status**: "healthy" si le service répond et que le moteur est initialisé, "unhealthy" sinon
    - **engine_initialized**: True si le moteur PaddleOCR est initialisé
    - **worker_pid**: Process ID du worker qui a répondu
    - **response_time_ms**: Temps de réponse en millisecondes
    - **message**: Message décrivant le résultat du health check
    
    **Error Handling:**
    - Retourne 503 Service Unavailable si le service OCR ne répond pas
    - Retourne 500 si une erreur inattendue se produit
    
    **Note:** Cet endpoint est bloquant et attend la réponse du microservice.
    Il peut être utilisé pour des health checks mais ne doit pas être appelé
    trop fréquemment pour éviter de surcharger le service.
    """
)
async def ocr_health_check() -> OCRHealthResponse:
    """
    Vérifie la santé du microservice OCR.
    
    Envoie une tâche de warm-up et attend le résultat pour valider
    que le service est opérationnel.
    
    Returns:
        OCRHealthResponse: Statut de santé du service OCR
    """
    start_time = time.time()
    
    try:
        # Envoyer une tâche de warm-up au microservice OCR
        message = warmup_ocr_worker.send()
        
        # Attendre le résultat avec un timeout
        try:
            # Timeout de 30 secondes pour le health check
            result = message.get_result(block=True, timeout=30000)
            response_time_ms = (time.time() - start_time) * 1000
            
            # Vérifier le résultat
            if result and result.get('status') == 'ready' and result.get('engine_initialized', False):
                logger.info(
                    f"Health check OCR réussi (PID: {result.get('worker_pid')}, temps: {response_time_ms:.2f}ms)",
                    extra={
                        "metrics": {
                            "status": "healthy",
                            "worker_pid": result.get('worker_pid'),
                            "response_time_ms": round(response_time_ms, 2)
                        }
                    }
                )
                return OCRHealthResponse(
                    status="healthy",
                    engine_initialized=True,
                    worker_pid=result.get('worker_pid'),
                    message="Le microservice OCR est opérationnel et le moteur PaddleOCR est initialisé",
                    response_time_ms=round(response_time_ms, 2)
                )
            else:
                # Le service a répondu mais le moteur n'est pas initialisé
                logger.warning(
                    f"Health check OCR: service répond mais moteur non initialisé",
                    extra={
                        "metrics": {
                            "status": "unhealthy",
                            "engine_initialized": result.get('engine_initialized', False) if result else False
                        }
                    }
                )
                return OCRHealthResponse(
                    status="unhealthy",
                    engine_initialized=result.get('engine_initialized', False) if result else False,
                    worker_pid=result.get('worker_pid') if result else None,
                    message="Le microservice OCR répond mais le moteur PaddleOCR n'est pas initialisé",
                    response_time_ms=round(response_time_ms, 2)
                )
                
        except ResultTimeout:
            response_time_ms = (time.time() - start_time) * 1000
            logger.error(
                f"Health check OCR: timeout après {response_time_ms:.2f}ms",
                extra={
                    "metrics": {
                        "status": "unhealthy",
                        "error": "timeout",
                        "response_time_ms": round(response_time_ms, 2)
                    }
                }
            )
            raise HTTPException(
                status_code=503,
                detail=f"Le microservice OCR n'a pas répondu dans le délai imparti (timeout: 30s)"
            )
        except Exception as e:
            response_time_ms = (time.time() - start_time) * 1000
            logger.error(
                f"Health check OCR: erreur lors de la récupération du résultat: {e}",
                exc_info=True,
                extra={
                    "metrics": {
                        "status": "unhealthy",
                        "error": str(e),
                        "response_time_ms": round(response_time_ms, 2)
                    }
                }
            )
            raise HTTPException(
                status_code=503,
                detail=f"Erreur lors de la communication avec le microservice OCR: {str(e)}"
            )
            
    except Exception as e:
        response_time_ms = (time.time() - start_time) * 1000
        logger.error(
            f"Health check OCR: erreur lors de l'envoi de la tâche: {e}",
            exc_info=True,
            extra={
                "metrics": {
                    "status": "unhealthy",
                    "error": str(e),
                    "response_time_ms": round(response_time_ms, 2)
                }
            }
        )
        raise HTTPException(
            status_code=503,
            detail=f"Impossible de communiquer avec le microservice OCR: {str(e)}"
        )


# ==========================================
# Dead Letter Queue (DLQ) Management
# ==========================================

class DLQMessage(BaseModel):
    """Modèle pour un message de la DLQ"""
    message_id: str = Field(..., description="ID unique du message")
    actor_name: str = Field(..., description="Nom de l'actor Dramatiq")
    args: List[Any] = Field(default_factory=list, description="Arguments du message")
    kwargs: Dict[str, Any] = Field(default_factory=dict, description="Arguments nommés du message")
    queue_name: str = Field(..., description="Nom de la queue")
    retries: int = Field(..., description="Nombre de tentatives effectuées")
    max_retries: int = Field(..., description="Nombre maximum de tentatives")
    timestamp: Optional[int] = Field(None, description="Timestamp du message")


class DLQListResponse(BaseModel):
    """Réponse pour la liste des messages DLQ"""
    total: int = Field(..., description="Nombre total de messages dans la DLQ")
    messages: List[DLQMessage] = Field(..., description="Liste des messages")
    limit: int = Field(..., description="Limite appliquée à la requête")


class DLQStatisticsResponse(BaseModel):
    """Statistiques sur la DLQ"""
    total_messages: int = Field(..., description="Nombre total de messages")
    by_actor: Dict[str, int] = Field(..., description="Répartition par actor")
    oldest_message: Optional[int] = Field(None, description="Timestamp du message le plus ancien")
    newest_message: Optional[int] = Field(None, description="Timestamp du message le plus récent")


class DLQReplayResponse(BaseModel):
    """Réponse pour le rejeu d'un message"""
    success: bool = Field(..., description="True si le message a été rejoué avec succès")
    message_id: str = Field(..., description="ID du message rejoué")
    message: str = Field(..., description="Message de statut")


@router.get(
    "/dlq/messages",
    response_model=DLQListResponse,
    summary="List Dead Letter Queue Messages",
    description="""
    Liste les messages dans la Dead Letter Queue (DLQ).
    
    Les messages dans la DLQ sont des tâches qui ont échoué après avoir atteint
    leur nombre maximum de tentatives (max_retries). Ces messages peuvent être
    analysés pour comprendre les erreurs et potentiellement rejoués.
    
    **Use Cases:**
    - Inspecter les tâches échouées pour debugging
    - Identifier les patterns d'erreurs récurrents
    - Préparer le rejeu de messages après correction d'un bug
    
    **Query Parameters:**
    - **limit** (optional, default: 100): Nombre maximum de messages à retourner
    
    **Response:**
    - **total**: Nombre total de messages dans la DLQ
    - **messages**: Liste des messages avec leurs métadonnées
    - **limit**: Limite appliquée à la requête
    """
)
async def list_dlq_messages(limit: int = 100) -> DLQListResponse:
    """
    Liste les messages dans la Dead Letter Queue.
    """
    from src.utils.dlq_manager import DLQManager
    
    try:
        dlq_manager = DLQManager()
        total = dlq_manager.get_dlq_length()
        messages_data = dlq_manager.list_messages(limit=limit)
        
        messages = [
            DLQMessage(
                message_id=msg['message_id'],
                actor_name=msg['actor_name'],
                args=msg.get('args', []),
                kwargs=msg.get('kwargs', {}),
                queue_name=msg.get('queue_name', 'default'),
                retries=msg.get('retries', 0),
                max_retries=msg.get('max_retries', 0),
                timestamp=msg.get('timestamp')
            )
            for msg in messages_data
        ]
        
        return DLQListResponse(
            total=total,
            messages=messages,
            limit=limit
        )
    except Exception as e:
        logger.error(f"Erreur lors de la récupération des messages DLQ: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Erreur lors de la récupération des messages DLQ: {str(e)}")


@router.get(
    "/dlq/statistics",
    response_model=DLQStatisticsResponse,
    summary="Get DLQ Statistics",
    description="""
    Retourne des statistiques sur la Dead Letter Queue.
    
    **Use Cases:**
    - Monitoring de la santé du système
    - Identification des actors qui échouent le plus souvent
    - Décision sur la nécessité d'intervention
    
    **Response:**
    - **total_messages**: Nombre total de messages dans la DLQ
    - **by_actor**: Répartition des messages par actor
    - **oldest_message**: Timestamp du message le plus ancien
    - **newest_message**: Timestamp du message le plus récent
    """
)
async def get_dlq_statistics() -> DLQStatisticsResponse:
    """
    Retourne des statistiques sur la DLQ.
    """
    from src.utils.dlq_manager import DLQManager
    
    try:
        dlq_manager = DLQManager()
        stats = dlq_manager.get_statistics()
        
        return DLQStatisticsResponse(
            total_messages=stats['total_messages'],
            by_actor=stats['by_actor'],
            oldest_message=stats.get('oldest_message'),
            newest_message=stats.get('newest_message')
        )
    except Exception as e:
        logger.error(f"Erreur lors de la récupération des statistiques DLQ: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Erreur lors de la récupération des statistiques DLQ: {str(e)}")


@router.post(
    "/dlq/replay/{message_id}",
    response_model=DLQReplayResponse,
    summary="Replay a DLQ Message",
    description="""
    Rejoue un message depuis la Dead Letter Queue.
    
    Le message est retiré de la DLQ et renvoyé dans la queue normale pour être
    traité à nouveau. Le compteur de retries est réinitialisé.
    
    **Use Cases:**
    - Rejouer une tâche après correction d'un bug
    - Réessayer une tâche qui a échoué à cause d'une erreur temporaire
    
    **Path Parameters:**
    - **message_id**: ID du message à rejouer
    
    **Response:**
    - **success**: True si le message a été rejoué avec succès
    - **message_id**: ID du message rejoué
    - **message**: Message de statut
    """
)
async def replay_dlq_message(message_id: str) -> DLQReplayResponse:
    """
    Rejoue un message depuis la DLQ.
    """
    from src.utils.dlq_manager import DLQManager
    
    try:
        dlq_manager = DLQManager()
        success = dlq_manager.replay_message(message_id)
        
        if success:
            return DLQReplayResponse(
                success=True,
                message_id=message_id,
                message=f"Message {message_id} rejoué avec succès"
            )
        else:
            return DLQReplayResponse(
                success=False,
                message_id=message_id,
                message=f"Impossible de rejouer le message {message_id}. Vérifiez qu'il existe dans la DLQ."
            )
    except Exception as e:
        logger.error(f"Erreur lors du rejeu du message {message_id}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Erreur lors du rejeu du message: {str(e)}")


@router.delete(
    "/dlq/messages/{message_id}",
    summary="Delete a DLQ Message",
    description="""
    Supprime définitivement un message de la Dead Letter Queue.
    
    **Use Cases:**
    - Supprimer des messages obsolètes ou non pertinents
    - Nettoyer la DLQ après analyse
    
    **Path Parameters:**
    - **message_id**: ID du message à supprimer
    
    **Response:**
    - Message de confirmation
    """
)
async def delete_dlq_message(message_id: str):
    """
    Supprime un message de la DLQ.
    """
    from src.utils.dlq_manager import DLQManager
    
    try:
        dlq_manager = DLQManager()
        success = dlq_manager.delete_message(message_id)
        
        if success:
            return {"success": True, "message": f"Message {message_id} supprimé de la DLQ"}
        else:
            raise HTTPException(
                status_code=404,
                detail=f"Message {message_id} non trouvé dans la DLQ"
            )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erreur lors de la suppression du message {message_id}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Erreur lors de la suppression du message: {str(e)}")


@router.delete(
    "/dlq/clear",
    summary="Clear Dead Letter Queue",
    description="""
    Vide complètement la Dead Letter Queue.
    
    ⚠️ **Attention**: Cette opération est irréversible. Tous les messages de la DLQ
    seront supprimés définitivement.
    
    **Use Cases:**
    - Nettoyer la DLQ après résolution d'un problème systémique
    - Réinitialiser la DLQ dans un environnement de test
    
    **Response:**
    - Nombre de messages supprimés
    """
)
async def clear_dlq():
    """
    Vide complètement la DLQ.
    """
    from src.utils.dlq_manager import DLQManager
    
    try:
        dlq_manager = DLQManager()
        deleted_count = dlq_manager.clear_dlq()
        
        return {
            "success": True,
            "message": f"DLQ vidée: {deleted_count} message(s) supprimé(s)",
            "deleted_count": deleted_count
        }
    except Exception as e:
        logger.error(f"Erreur lors du vidage de la DLQ: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Erreur lors du vidage de la DLQ: {str(e)}")


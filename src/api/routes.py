"""
API routes for document analysis
"""

from fastapi import APIRouter, UploadFile, File, HTTPException, BackgroundTasks
from fastapi.responses import JSONResponse
from typing import Optional, Dict, Any, List
from pydantic import BaseModel, Field
import os
from pathlib import Path

from src.services.processing_service import ProcessingService
from src.services.task_manager import get_task_manager, TaskStatus
from src.api.dependencies import (
    get_preprocessing_normalizer,
    get_geometry_normalizer,
    get_app_config
)
from src.pipeline.preprocessing import PreprocessingNormalizer
from src.pipeline.geometry import GeometryNormalizer
from src.utils.logger import get_logger, get_request_id
from src.utils.exceptions import (
    GeometryError, 
    PreprocessingError, 
    ModelLoadingError, 
    ImageProcessingError,
    PipelineError
)
from typing import Annotated
from fastapi import Depends

logger = get_logger(__name__)

router = APIRouter()

# Initialize task manager (singleton)
task_manager = get_task_manager()


# ==========================================
# Pydantic Response Models
# ==========================================

class OutputFiles(BaseModel):
    """Output file paths generated by the pipeline"""
    transformed: str = Field(..., description="Path to the transformed/processed image")
    original: Optional[str] = Field(None, description="Path to the original image (if saved)")
    transform_file: str = Field(..., description="Path to the JSON file containing applied transformations")
    qa_file: str = Field(..., description="Path to the JSON file containing QA flags")


class Metadata(BaseModel):
    """Processing metadata and applied transformations"""
    crop_applied: bool = Field(..., description="Whether intelligent cropping was applied")
    deskew_applied: bool = Field(..., description="Whether angle correction (deskew) was applied")
    rotation_applied: bool = Field(..., description="Whether orientation correction was applied")
    orientation_angle: float = Field(..., description="Detected orientation angle in degrees")
    capture_type: str = Field(..., description="Detected capture type: 'SCAN' or 'PHOTO'")
    processing_time: float = Field(..., description="Total processing time in seconds")


class TempDirs(BaseModel):
    """Temporary directories used during processing"""
    input: str = Field(..., description="Temporary directory for input files")
    output: str = Field(..., description="Temporary directory for output files")
    preprocessing: str = Field(..., description="Temporary directory for preprocessing files")


class PipelineStages(BaseModel):
    """Status of each pipeline stage"""
    preprocessing: str = Field(..., description="Status of preprocessing stage")
    geometry: str = Field(..., description="Status of geometry normalization stage")
    colometry: str = Field(..., description="Status of colometry normalization stage")
    features: str = Field(..., description="Status of feature extraction stage")


class GeometryResponse(BaseModel):
    """Response model for geometry normalization endpoint"""
    status: str = Field(..., description="Processing status: 'success' or 'error'")
    input_filename: Optional[str] = Field(None, description="Name of the uploaded input file")
    output_files: OutputFiles = Field(..., description="Paths to generated output files")
    metadata: Metadata = Field(..., description="Processing metadata and applied transformations")
    qa_flags: Dict[str, Any] = Field(..., description="Quality assurance flags indicating potential issues")
    temp_dirs: TempDirs = Field(..., description="Temporary directories used during processing")


class AnalyzeResponse(BaseModel):
    """Response model for full document analysis endpoint"""
    status: str = Field(..., description="Processing status: 'success' or 'error'")
    input_filename: Optional[str] = Field(None, description="Name of the uploaded input file")
    output_files: OutputFiles = Field(..., description="Paths to generated output files")
    metadata: Metadata = Field(..., description="Processing metadata and applied transformations")
    qa_flags: Dict[str, Any] = Field(..., description="Quality assurance flags indicating potential issues")
    pipeline_stages: PipelineStages = Field(..., description="Status of each pipeline stage")
    temp_dirs: TempDirs = Field(..., description="Temporary directories used during processing")


class NotImplementedResponse(BaseModel):
    """Response model for not yet implemented endpoints"""
    status: str = Field(..., description="Status indicating the endpoint is not implemented")
    message: str = Field(..., description="Message explaining that the feature is not yet available")


class StatusResponse(BaseModel):
    """Response model for pipeline status endpoint"""
    status: str = Field(..., description="Overall pipeline status")
    stages: List[str] = Field(..., description="List of available pipeline stages")


class TaskResponse(BaseModel):
    """Response model for task creation"""
    task_id: str = Field(..., description="Unique task identifier")
    status: str = Field(..., description="Current task status")
    message: str = Field(..., description="Human-readable message")


class TaskStatusResponse(BaseModel):
    """Response model for task status query"""
    task_id: str = Field(..., description="Task identifier")
    status: str = Field(..., description="Current task status")
    created_at: Optional[str] = Field(None, description="Task creation timestamp")
    started_at: Optional[str] = Field(None, description="Task start timestamp")
    completed_at: Optional[str] = Field(None, description="Task completion timestamp")
    result: Optional[Dict[str, Any]] = Field(None, description="Task result (if completed)")
    error: Optional[str] = Field(None, description="Error message (if failed)")
    filename: Optional[str] = Field(None, description="Original filename")


# Helper function for background task execution
def execute_processing_task_sync(
    task_id: str,
    file_path: str,
    filename: str,
    temp_dirs: Dict[str, str],
    preprocessing_normalizer,
    geometry_normalizer,
    request_id: Optional[str] = None,
    is_full_analysis: bool = False
) -> None:
    """
    Execute processing task synchronously (to be run in background).
    
    Args:
        task_id: Task ID
        file_path: Path to input file
        filename: Original filename
        temp_dirs: Temporary directories
        preprocessing_normalizer: Preprocessing normalizer (injected)
        geometry_normalizer: Geometry normalizer (injected)
        request_id: Request ID for correlation (optional, will use context if not provided)
        is_full_analysis: Whether to run full analysis or just geometry
    """
    # Importer ici pour éviter les imports circulaires
    from src.utils.logger import set_request_id
    
    # Définir le request_id dans le contexte pour cette tâche
    if request_id:
        set_request_id(request_id)
    
    try:
        # Create service with injected dependencies
        service = ProcessingService(
            preprocessing_normalizer=preprocessing_normalizer,
            geometry_normalizer=geometry_normalizer
        )
        
        # Update task status to processing
        task_manager.update_task_status(task_id, TaskStatus.PROCESSING)
        logger.info(f"Début du traitement de la tâche {task_id}")
        
        # Run processing
        if is_full_analysis:
            result = service.process_full_analysis(
                file_path,
                filename,
                temp_dirs
            )
        else:
            result = service.process_geometry(
                file_path,
                filename,
                temp_dirs
            )
        
        # Update task with result
        task_manager.update_task_status(task_id, TaskStatus.COMPLETED, result=result)
        logger.info(f"Tâche {task_id} complétée avec succès")
        
        # Schedule cleanup after task completion
        # Note: In production, this should be handled by a proper task queue system
        cleanup_temp_files(temp_dirs)
        
    except Exception as e:
        error_msg = str(e)
        logger.error(f"Erreur lors de l'exécution de la tâche {task_id}: {e}", exc_info=True)
        task_manager.update_task_status(task_id, TaskStatus.FAILED, error=error_msg)
        
        # Cleanup on error as well
        cleanup_temp_files(temp_dirs)


def cleanup_temp_files(temp_dirs: Dict[str, str]) -> None:
    """
    Clean up temporary files after response is sent.
    This function is designed to be called as a background task.
    
    Args:
        temp_dirs: Dictionary with temporary directory paths
    """
    # Create a temporary service instance for cleanup (no dependencies needed)
    service = ProcessingService()
    service.cleanup_temp_directories(temp_dirs)
    logger.info("Nettoyage des fichiers temporaires terminé")


@router.post(
    "/analyze",
    response_model=TaskResponse,
    status_code=202,
    summary="Complete Document Analysis (Async)",
    description="""
    Submits a document for complete analysis. Returns immediately with a task_id.
    Use GET /results/{task_id} to check the status and retrieve results.
    
    **Current Implementation:**
    - Preprocessing: Image enhancement, contrast improvement, and capture type classification (SCAN vs PHOTO)
    - Geometry Normalization: Intelligent cropping, deskew (angle correction), and orientation correction
    
    **Future Stages (Not Yet Implemented):**
    - Colometry Normalization: Column structure normalization
    - Feature Extraction: OCR text extraction and checkbox detection
    
    **Input:**
    - Accepts image files (PNG, JPG, JPEG, TIFF, BMP) or PDF files
    - Maximum file size depends on server configuration
    
    **Response:**
    - Returns 202 Accepted with a task_id
    - Processing happens asynchronously in the background
    - Use GET /results/{task_id} to check status and get results
    
    **Error Handling:**
    - Returns 400 for invalid file formats or empty files
    - Returns 500 for server errors
    """
)
async def analyze_document(
    background_tasks: BackgroundTasks,
    preprocessing_normalizer: Annotated[PreprocessingNormalizer, Depends(get_preprocessing_normalizer)],
    geometry_normalizer: Annotated[GeometryNormalizer, Depends(get_geometry_normalizer)],
    file: UploadFile = File(...)
) -> TaskResponse:
    """
    Submit document for complete analysis (asynchronous).
    """
    try:
        # Create service with injected dependencies
        service = ProcessingService(
            preprocessing_normalizer=preprocessing_normalizer,
            geometry_normalizer=geometry_normalizer
        )
        
        # 1. Validate file
        file_content = await file.read()
        file_extension, error = service.validate_file(file.filename, len(file_content))
        if error:
            raise HTTPException(status_code=400, detail=error)
        
        # 2. Create task
        task_id = task_manager.create_task(file.filename)
        
        # 3. Create temporary directories
        temp_dirs = service.create_temp_directories(prefix="analyze_")
        
        # 4. Save uploaded file
        temp_file_path = service.save_uploaded_file(
            file_content,
            file.filename or f"upload{file_extension}",
            temp_dirs['input']
        )
        
        # 5. Schedule background task for processing
        # Note: For production, consider using Celery or Dramatiq for better task management
        # Récupérer le request_id pour le propager à la tâche asynchrone
        current_request_id = get_request_id()
        background_tasks.add_task(
            execute_processing_task_sync,
            task_id=task_id,
            file_path=temp_file_path,
            filename=file.filename,
            temp_dirs=temp_dirs,
            preprocessing_normalizer=preprocessing_normalizer,
            geometry_normalizer=geometry_normalizer,
            request_id=current_request_id,
            is_full_analysis=True
        )
        
        # 6. Schedule cleanup after response is sent (with delay to allow task to complete)
        # Note: In production, cleanup should happen after task completion, not immediately
        # For now, we keep files until task is completed
        # background_tasks.add_task(cleanup_temp_files, temp_dirs)
        
        logger.info(f"Tâche d'analyse créée: {task_id} pour le fichier {file.filename}")
        
        return TaskResponse(
            task_id=task_id,
            status=TaskStatus.PENDING.value,
            message="Document analysis task created. Use GET /results/{task_id} to check status."
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erreur inattendue lors de la création de la tâche: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Erreur interne lors de la création de la tâche: {str(e)}"
        )


@router.post(
    "/pipeline/colometry",
    response_model=NotImplementedResponse,
    status_code=501,
    summary="Column Structure Normalization",
    description="""
    Normalizes the column structure of a document.
    
    **Status:** Not yet implemented
    
    **Planned Functionality:**
    This endpoint will normalize the column layout of documents, ensuring consistent column widths,
    detecting multi-column layouts, and standardizing column boundaries.
    
    **Future Parameters:**
    - Expected number of columns
    - Minimum column width thresholds
    - Column spacing normalization
    
    **Note:** This feature is planned for future implementation. Check the pipeline status endpoint
    for current implementation status.
    """
)
async def pipeline_colometry(file: UploadFile = File(...)) -> NotImplementedResponse:
    """
    Column structure normalization endpoint (not yet implemented).
    """
    return NotImplementedResponse(
        status="not_implemented",
        message="Column structure normalization - Not yet implemented"
    )


@router.post(
    "/pipeline/geometry",
    response_model=GeometryResponse,
    summary="Document Geometry Normalization",
    description="""
    Accepts an image or PDF file, applies preprocessing (enhancement, contrast improvement, classification)
    then performs geometry normalization (crop, deskew, rotation).
    
    Returns processing metadata and paths to generated files.
    
    **Processing Steps:**
    
    1. **Preprocessing:**
       - Image enhancement and contrast improvement
       - Capture type classification (SCAN vs PHOTO)
       - White level analysis to determine document type
    
    2. **Geometry Normalization:**
       - **Intelligent Cropping:** Automatically crops the document from background using doctr
       - **Deskew (Angle Correction):** Corrects document rotation/angle skew using Hough line detection
       - **Orientation Correction:** Detects and corrects document orientation (0°, 90°, 180°, 270°) using ONNX model
    
    3. **Quality Assurance:**
       - Generates QA flags for potential issues (low contrast, overcrop risk, small resolution, etc.)
       - Validates minimum quality standards
    
    **Input:**
    - Accepts image files (PNG, JPG, JPEG, TIFF, BMP) or PDF files
    - PDFs are converted to images at 300 DPI
    
    **Output:**
    - **transformed:** Path to the final processed image after all transformations
    - **original:** Path to the original image (if saved)
    - **transform_file:** JSON file containing all applied transformations
    - **qa_file:** JSON file containing quality assurance flags
    
    **Metadata Includes:**
    - Which transformations were applied (crop, deskew, rotation)
    - Detected orientation angle
    - Capture type (SCAN or PHOTO)
    - Processing time
    
    **Error Handling:**
    - Returns 400 for invalid file formats or empty files
    - Returns 500 for processing errors (preprocessing, geometry, model loading, etc.)
    """
)
async def pipeline_geometry(
    background_tasks: BackgroundTasks,
    preprocessing_normalizer: Annotated[PreprocessingNormalizer, Depends(get_preprocessing_normalizer)],
    geometry_normalizer: Annotated[GeometryNormalizer, Depends(get_geometry_normalizer)],
    file: UploadFile = File(...)
) -> GeometryResponse:
    """
    Process document geometry normalization (synchronous for backward compatibility).
    """
    # Create service with injected dependencies
    service = ProcessingService(
        preprocessing_normalizer=preprocessing_normalizer,
        geometry_normalizer=geometry_normalizer
    )
    
    temp_dirs = None
    
    try:
        # 1. Validate file
        file_content = await file.read()
        file_extension, error = service.validate_file(file.filename, len(file_content))
        if error:
            raise HTTPException(status_code=400, detail=error)
        
        # 2. Create temporary directories
        temp_dirs = service.create_temp_directories(prefix="geometry_")
        
        # 3. Save uploaded file
        temp_file_path = service.save_uploaded_file(
            file_content,
            file.filename or f"upload{file_extension}",
            temp_dirs['input']
        )
        
        # 4. Process document
        response_data = service.process_geometry(
            temp_file_path,
            file.filename,
            temp_dirs
        )
        
        # 5. Schedule cleanup after response is sent
        background_tasks.add_task(cleanup_temp_files, temp_dirs)
        
        return response_data
        
    except HTTPException:
        raise
    except (PreprocessingError, GeometryError, ModelLoadingError, ImageProcessingError) as e:
        logger.error(f"Erreur du pipeline: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Erreur lors du traitement: {str(e)}"
        )
    except ValueError as e:
        logger.error(f"Erreur de validation: {e}", exc_info=True)
        raise HTTPException(status_code=400, detail=str(e))
    except PipelineError as e:
        logger.error(f"Erreur du pipeline: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Erreur du pipeline: {str(e)}"
        )
    except Exception as e:
        logger.error(f"Erreur inattendue lors du traitement géométrique: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Erreur interne lors du traitement: {str(e)}"
        )
    finally:
        # Cleanup is handled by background task, but ensure cleanup on error
        if temp_dirs:
            # Only cleanup on error, otherwise let background task handle it
            pass


@router.post(
    "/pipeline/features",
    response_model=NotImplementedResponse,
    status_code=501,
    summary="Feature Extraction",
    description="""
    Extracts features from documents including OCR text and checkbox detection.
    
    **Status:** Not yet implemented
    
    **Planned Functionality:**
    This endpoint will extract structured information from documents:
    
    - **OCR (Optical Character Recognition):**
      - Text extraction from images using Tesseract or similar OCR engine
      - Multi-language support (configurable default language)
      - Confidence-based filtering of recognized text
      - Word and line-level text extraction with coordinates
    
    - **Checkbox Detection:**
      - Automatic detection of checkbox elements in documents
      - Classification of checkboxes as checked or unchecked
      - Confidence thresholds for detection and state classification
    
    **Future Parameters:**
    - OCR language settings (default: French)
    - OCR confidence thresholds
    - Checkbox detection confidence thresholds
    - Checkbox checked state thresholds
    
    **Note:** This feature is planned for future implementation. Check the pipeline status endpoint
    for current implementation status.
    """
)
async def pipeline_features(file: UploadFile = File(...)) -> NotImplementedResponse:
    """
    Feature extraction endpoint (not yet implemented).
    """
    return NotImplementedResponse(
        status="not_implemented",
        message="Feature extraction - Not yet implemented"
    )


@router.get(
    "/results/{task_id}",
    response_model=TaskStatusResponse,
    summary="Get Task Status and Results",
    description="""
    Retrieve the status and results of an asynchronous processing task.
    
    **Use Cases:**
    - Check if a task is still processing
    - Retrieve results when task is completed
    - Get error information if task failed
    
    **Response Status:**
    - **pending:** Task created but not yet started
    - **processing:** Task is currently being processed
    - **completed:** Task completed successfully, results available
    - **failed:** Task failed, error message available
    
    **Response Fields:**
    - **task_id:** Unique task identifier
    - **status:** Current task status
    - **created_at:** When the task was created
    - **started_at:** When processing started (if started)
    - **completed_at:** When processing completed (if completed)
    - **result:** Complete processing results (if completed)
    - **error:** Error message (if failed)
    - **filename:** Original filename
    
    **Error Handling:**
    - Returns 404 if task_id is not found
    """
)
async def get_task_results(task_id: str) -> TaskStatusResponse:
    """
    Get the status and results of an asynchronous processing task.
    """
    task = task_manager.get_task(task_id)
    
    if not task:
        raise HTTPException(
            status_code=404,
            detail=f"Task {task_id} not found"
        )
    
    return TaskStatusResponse(**task.to_dict())


@router.get(
    "/pipeline/status",
    response_model=StatusResponse,
    summary="Pipeline Status",
    description="""
    Returns the current status of the document analysis pipeline.
    
    **Use Cases:**
    - Check which pipeline stages are available and implemented
    - Verify API health and readiness
    - Determine which endpoints are functional
    
    **Response:**
    - **status:** Overall pipeline status (e.g., "ready", "maintenance")
    - **stages:** List of available pipeline stages
    
    **Available Stages:**
    - **preprocessing:** Image enhancement and capture type classification (implemented)
    - **geometry:** Geometry normalization - crop, deskew, rotation (implemented)
    - **colometry:** Column structure normalization (not yet implemented)
    - **features:** Feature extraction - OCR and checkbox detection (not yet implemented)
    
    This endpoint does not require any input and can be used for health checks.
    """
)
async def pipeline_status() -> StatusResponse:
    """
    Returns the current status of the document analysis pipeline.
    """
    return StatusResponse(
        status="ready",
        stages=["preprocessing", "geometry", "colometry", "features"]
    )

